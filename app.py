"""
PR Size Score Dashboard - Streamlit App
"""

import base64
import json
import subprocess
from pathlib import Path

import pandas as pd
import plotly.express as px
import streamlit as st
import yaml

# „Éö„Éº„Ç∏Ë®≠ÂÆö
st.set_page_config(
    page_title="PR Size Score Dashboard",
    page_icon="üìä",
    layout="wide",
)

# „Éë„ÇπË®≠ÂÆö
PROJECT_DIR = Path(__file__).parent
DATA_PATH = PROJECT_DIR / "data" / "pr_size_scores.jsonl"
CONFIG_PATH = PROJECT_DIR / "config.yaml"

# ÁõÆÊ®ô„É©„Ç§„É≥ÔºàÂ∞è„Åï„ÅÑPR„ÅÆÁõÆÂÆâÔºâ
SCORE_TARGET = 10.0


def fetch_metrics() -> tuple[bool, str]:
    """GitHub API„Åã„Çâ„Éá„Éº„Çø„ÇíÂèñÂæó"""
    with open(CONFIG_PATH) as f:
        config = yaml.safe_load(f)

    repositories = config.get("repositories", [])
    all_records: dict[str, dict] = {}
    messages = []

    for repo in repositories:
        result = subprocess.run(
            ["gh", "api", f"repos/{repo}/contents/metrics/pr_size_scores.jsonl", "--jq", ".content"],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            messages.append(f"- {repo}: „Éá„Éº„Çø„Å™„Åó")
            continue

        try:
            decoded = base64.b64decode(result.stdout.strip()).decode("utf-8")
            count = 0
            for line in decoded.strip().split("\n"):
                if line:
                    record = json.loads(line)
                    key = f"{record['repo']}:{record['pr_number']}"
                    all_records[key] = record
                    count += 1
            messages.append(f"- {repo}: {count}‰ª∂")
        except Exception as e:
            messages.append(f"- {repo}: „Ç®„É©„Éº ({e})")

    # ‰øùÂ≠ò
    DATA_PATH.parent.mkdir(parents=True, exist_ok=True)
    sorted_records = sorted(all_records.values(), key=lambda r: r.get("merged_at", ""))

    with open(DATA_PATH, "w", encoding="utf-8") as f:
        for record in sorted_records:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

    return True, f"ÂèñÂæóÂÆå‰∫Ü: {len(sorted_records)}‰ª∂\n" + "\n".join(messages)


def load_data() -> pd.DataFrame:
    """JSONL„Åã„Çâ„Éá„Éº„Çø„ÇíË™≠„ÅøËæº„ÇÄ"""
    if not DATA_PATH.exists():
        return pd.DataFrame()

    records = []
    with open(DATA_PATH, encoding="utf-8") as f:
        for line in f:
            if line.strip():
                records.append(json.loads(line))

    if not records:
        return pd.DataFrame()

    df = pd.DataFrame(records)
    df["merged_at"] = pd.to_datetime(df["merged_at"])
    df["date"] = df["merged_at"].dt.date
    df["week"] = df["merged_at"].dt.to_period("W").astype(str)
    return df


def main():
    st.title("üìä PR Size Score Dashboard")

    # „Çµ„Ç§„Éâ„Éê„Éº: „Éá„Éº„ÇøÊõ¥Êñ∞
    st.sidebar.header("„Éá„Éº„ÇøÊõ¥Êñ∞")
    if st.sidebar.button("„Éá„Éº„Çø„ÇíÂèñÂæó", type="primary", use_container_width=True):
        with st.spinner("GitHub API„Åã„Çâ„Éá„Éº„Çø„ÇíÂèñÂæó‰∏≠..."):
            success, message = fetch_metrics()
            if success:
                st.sidebar.success(message)
                st.cache_data.clear()
                st.rerun()
            else:
                st.sidebar.error(message)

    # „Éá„Éº„ÇøË™≠„ÅøËæº„Åø
    df = load_data()

    if df.empty:
        st.warning("„Éá„Éº„Çø„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Çµ„Ç§„Éâ„Éê„Éº„ÅÆ„Äå„Éá„Éº„Çø„ÇíÂèñÂæó„Äç„Éú„Çø„É≥„Çí„ÇØ„É™„ÉÉ„ÇØ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
        return

    # „Çµ„Ç§„Éâ„Éê„Éº: „Éï„Ç£„É´„Çø
    st.sidebar.header("„Éï„Ç£„É´„Çø")

    repos = ["All"] + sorted(df["repo"].unique().tolist())
    selected_repo = st.sidebar.selectbox("„É™„Éù„Ç∏„Éà„É™", repos)

    authors = ["All"] + sorted(df["author"].unique().tolist())
    selected_author = st.sidebar.selectbox("ËëóËÄÖ", authors)

    # „Éï„Ç£„É´„ÇøÈÅ©Áî®
    filtered_df = df.copy()
    if selected_repo != "All":
        filtered_df = filtered_df[filtered_df["repo"] == selected_repo]
    if selected_author != "All":
        filtered_df = filtered_df[filtered_df["author"] == selected_author]

    # ÈÅéÂéª7Êó•Èñì„Å®ÂâçÈÄ±„ÅÆ„Éá„Éº„Çø„ÇíÊäΩÂá∫
    today = pd.Timestamp.now(tz="UTC").normalize()
    last_7_days = filtered_df[filtered_df["merged_at"] >= today - pd.Timedelta(days=7)]
    prev_7_days = filtered_df[
        (filtered_df["merged_at"] >= today - pd.Timedelta(days=14)) &
        (filtered_df["merged_at"] < today - pd.Timedelta(days=7))
    ]

    # „É°„Éà„É™„ÇØ„ÇπÔºàÈÅéÂéª7Êó•ÈñìÂÆüÁ∏æ„ÉªÂâçÈÄ±ÊØîÔºâ
    st.header("„Çµ„Éû„É™„ÉºÔºàÈÅéÂéª7Êó•ÈñìÔºâ")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        current_count = len(last_7_days)
        prev_count = len(prev_7_days)
        delta_count = current_count - prev_count if prev_count > 0 else None
        st.metric("PRÊï∞", current_count, delta=delta_count)
    with col2:
        current_sum = last_7_days["size_score"].sum() if len(last_7_days) > 0 else 0
        prev_sum = prev_7_days["size_score"].sum() if len(prev_7_days) > 0 else None
        delta_sum = f"{current_sum - prev_sum:.2f}" if prev_sum is not None else None
        st.metric("ÂêàË®à„Çπ„Ç≥„Ç¢", f"{current_sum:.2f}", delta=delta_sum, delta_color="inverse")
    with col3:
        current_loc = last_7_days["loc"].mean() if len(last_7_days) > 0 else 0
        prev_loc = prev_7_days["loc"].mean() if len(prev_7_days) > 0 else None
        delta_loc = f"{current_loc - prev_loc:.0f}" if prev_loc is not None else None
        st.metric("Âπ≥ÂùáLOC", f"{current_loc:.0f}", delta=delta_loc, delta_color="inverse")
    with col4:
        current_small = (
            len(last_7_days[last_7_days["size_score"] <= SCORE_TARGET]) / len(last_7_days) * 100
            if len(last_7_days) > 0 else 0
        )
        prev_small = (
            len(prev_7_days[prev_7_days["size_score"] <= SCORE_TARGET]) / len(prev_7_days) * 100
            if len(prev_7_days) > 0 else None
        )
        delta_small = f"{current_small - prev_small:.1f}%" if prev_small is not None else None
        st.metric("Â∞èË¶èÊ®°PRÁéá", f"{current_small:.1f}%", delta=delta_small, help=f"„Çπ„Ç≥„Ç¢{SCORE_TARGET}‰ª•‰∏ã„ÅÆPRÂâ≤Âêà")

    # „Çπ„Ç≥„Ç¢Êé®Áßª„Ç∞„É©„ÉïÔºàÊó•Ê¨°ÂêàË®à„Éª„É™„Éù„Ç∏„Éà„É™Âà•Á©ç„Åø‰∏ä„ÅíÔºâ
    st.header("„Çπ„Ç≥„Ç¢Êé®Áßª")

    if len(filtered_df) > 0:
        daily_scores = filtered_df.groupby(["date", "repo"])["size_score"].sum().reset_index()
        daily_scores["date"] = pd.to_datetime(daily_scores["date"])

        fig = px.bar(
            daily_scores,
            x="date",
            y="size_score",
            color="repo",
            title="Êó•Ê¨°ÂêàË®à„Çπ„Ç≥„Ç¢Ôºà„É™„Éù„Ç∏„Éà„É™Âà•Á©ç„Åø‰∏ä„ÅíÔºâ",
            labels={"size_score": "ÂêàË®à„Çπ„Ç≥„Ç¢", "date": "Êó•‰ªò", "repo": "„É™„Éù„Ç∏„Éà„É™"},
        )
        fig.update_layout(barmode="stack")
        st.plotly_chart(fig, use_container_width=True)

    # „É™„Éù„Ç∏„Éà„É™Âà•ÊØîËºÉÔºàÈÅéÂéª7Êó•Èñì„ÉªÂÜÜ„Ç∞„É©„ÉïÔºâ
    st.header("„É™„Éù„Ç∏„Éà„É™Âà•ÊØîËºÉÔºàÈÅéÂéª7Êó•ÈñìÔºâ")

    if len(last_7_days) > 0 and selected_repo == "All":
        repo_scores = last_7_days.groupby("repo")["size_score"].sum().reset_index()

        fig3 = px.pie(
            repo_scores,
            values="size_score",
            names="repo",
            title="„É™„Éù„Ç∏„Éà„É™Âà• ÂêàË®à„Çπ„Ç≥„Ç¢Ââ≤ÂêàÔºàÈÅéÂéª7Êó•ÈñìÔºâ",
        )
        fig3.update_traces(textposition="inside", textinfo="percent+label")
        st.plotly_chart(fig3, use_container_width=True)

    # PR‰∏ÄË¶ß
    st.header("PR‰∏ÄË¶ß")

    if len(filtered_df) > 0:
        display_df = filtered_df[[
            "repo", "pr_number", "merged_at", "author",
            "additions", "deletions", "loc", "changed_files", "size_score"
        ]].sort_values("merged_at", ascending=False)

        st.dataframe(
            display_df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "pr_number": st.column_config.NumberColumn("PR#", format="%d"),
                "merged_at": st.column_config.DatetimeColumn("„Éû„Éº„Ç∏Êó•ÊôÇ", format="YYYY-MM-DD HH:mm"),
                "size_score": st.column_config.NumberColumn("„Çπ„Ç≥„Ç¢", format="%.2f"),
            }
        )


if __name__ == "__main__":
    main()
